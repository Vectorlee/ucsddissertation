
\subsection{Experiment Design}
\label{sec:methtrain}

Previously, I described the ideal model of the measurement method,
which explained the idea and workflow at a high-level. However, this model
does not take into consideration packet loss or extra traffic at the
{\reflector}. As one would expect, these assumptions are unrealistic in a real
world scenario, as packet loss can happen at many stages along the path.
Furthermore, there is no guarantee that the host with an open port online
will not receive any other traffic. Thus, to perform the measurement in the real
world, I need to take all these factors into consideration and make sure
that the analysis model is robust to these real world uncertainties.

Moreover, the detection methodology also needs to be \textit{efficient},
\textit{accurate}, and have \textit{low overhead}. Since I need to
measure every pair of \texttt{({\reflector}, IP)}, which is a
very large number, and the blacklist content changes rapidly, the detection
method needs to be efficient so that I can finish the measurement in a
reasonable amount of time. The method should also have a low false positive
and false negative rate, so I can be confident about the result. Finally, it
should require as few packets as possible, both to meet network bandwidth
limitations on the measurement machine side and reduce impact on
{\reflectors}.

I define a \textit{trial} as a single measurement that tests if a
{\reflector} blocks a blacklist IP. Figure~\ref{fig:design_implementation}
shows the process of one trial in detail. 
The solid blue lines are the \textit{probe
packets}. Dashed red lines are the \textit{spoofed packets}. The spoofed
packets impersonating blacklist IPs trigger the increase of the
{\reflectors}' IP ID. The IP ID of responses to probe packets are used to
determine blocking behavior.
For each trial, the measurement
machine sends five consecutive \textit{probe packets} to the {\reflector},
with each packet being sent one second apart. In the experiment, the probe
packets are TCP SYN-ACK packets and I get IP IDs from response RST packets.
Between the third and fourth probe packets, the measurement machine sends
five \textit{spoofed packets}, also TCP SYN-ACK, with source IPs equal to the
blacklist IP. And between the fourth and the fifth probe packets, it sends
another five spoofed packets. Each time I send the five spoofed packets, I
send them 0.15 second apart consecutively, spreading them across the one-second
window between two IP ID probes.

\begin{figure}[t]
\centering
\includegraphics[width=0.85\columnwidth]{data_usage/images/croped_design_implementation.pdf}
\caption{Blocking detection methodology.}
\label{fig:design_implementation}
\end{figure}

Now, I inspect the increases between the IP IDs for the packets received by the
measurement machine. In an ideal world, when there is no other traffic
generated by the {\reflector}, and no packet loss during the measurement, one
should observe that the IP ID increases between consecutive probes by exactly
1, and for the last two deltas, since I send the spoofed packets in between
the probe packets, the final IP ID increases will be different based on the
host's blocking behavior.

If the {\reflector} does not block the blacklist IP, then
I will observe an IP ID increase sequence in the received RST responses as:
\[[\hspace{0.1cm} +1, +1, +6, +6 \hspace{0.1cm}]\]
Here the last two deltas are +6 since the {\reflector} does not block the
blacklist IP and thus responds to spoofed packets, causing IP ID to increase by
5, and the probe packet causes it to increase by another 1, which together make +6.

On the other hand, if the {\reflector} blocks the blacklist IP, then I will see an IP
ID increase sequence as:
\[ [\hspace{0.1cm} +1, +1, +1, +1 \hspace{0.1cm}] \]
Here the last two deltas are +1 since the {\reflector} blocks the blacklist IP,
leading to no extra change in IP ID.
%\textcolor{red}{maybe have an example of what happens when I do not see ideal conditions}

The first three probes --- corresponding to the first two IP ID deltas --- act as a
control. The last two ``probe and spoof'' patterns perform the actual measurement.
Seeing the initial two ``+1'' indicates this host is in a quiet period --- no
extra network traffic. Therefore, I can be more confident that the following
IP ID jump (``+6'' in this case) is because of the experiment. However,
while I present the choice of the numbers in the experiment as fait accompli,
there is a rationale behind the choice of numbers which I discuss in Section~\ref{subsec:fpfn_analysis}.

\subsubsection{Inference Criteria}
%\textbf{Inference Criteria: }
Now I discuss how to infer whether a {\reflector} is blocking a blacklist IP
in the real world. I have limited vantage points from the measurement machine, as
such, and my information is limited to the IP IDs I see from the {\reflector}.
Therefore, I would like to be very conservative when making a judgment. In this
measurement, my approach is to try the same trial, between a {\reflector} and a
blacklist IP, many times until I get a ``perfect signal'' --- a response
which matches all the criteria below:

\begin{enumerate}
    \item The measurement machine received exactly five RST responses from the {\reflector}.
    \item The five responses are received one second apart consecutively.
    \item The IP ID increase sequence in the five responses are either [+1, +1, +6, +6],
    which I will conclude as no blocking, or [+1, +1, +1, +1], which I will
    conclude as blocking.
    \item If any of the above three criteria are not met, I repeat the same experiment
    again. I repeat up to 15 times before giving up.
\end{enumerate}

Essentially, the first requirement ensures no packet loss. The second
requirement ensures responses I received reflect the real IP ID changes in
the {\reflector}. The Internet does not guarantee the order of packet arrival.
Although I send one probe packet per second, and send the spoofed
packets in between of the probe packets, these packets might not arrive at
the {\reflector} in the same order. There is a similar case for response packets.
Therefore, the IP ID sequence I get from the response packets
might not represent the real order of IP ID changes in the host. Requiring
that
the received response packets are also close to 1 second apart, I minimize
the probability of the reordered packets. In my experiment, I enforce that
the response packets can not be less than 0.85 or more than 1.15 seconds
apart.

The third requirement is the core of my inference logic. I want to be
conservative when concluding whether there is blocking or not, so I will
make the judgment only when I observe an IP ID increase sequence [+1, +1, +1,
+1] or [+1, +1, +6, +6], and ignore everything else. Then if I saw a
sequence of [+1, +1, +1, +1] but the host is not blocking the blacklist IP,
that would mean all the 10 spoofed packets were lost during the transmit. On
the other hand, if I see [+1, +1, +6, +6] and the host is actually blocking
the blacklist IP, then that would mean during the experiment, there are
exactly five extra packets generated by the host during each of the last two
windows. Both cases are very unlikely to happen, and I will show a concrete
analysis of false positives and false negatives in the remainder of this section.


\subsubsection{False Positive and False Negative Analysis}
\label{subsec:fpfn_analysis}
%\textbf{False Positive and False Negative Analysis: }
For the experiment, a ``false positive'' is when a host is not
blocking a blacklist IP, but I mistakenly conclude it is blocking. On the
other hand, a ``false negative'' is when a host is blocking a
blacklist IP, but I mistakenly conclude it is not blocking. With
{\reflectors} being collected, I can empirically evaluate the
probability of a false positive or a false negative precisely.

For false positive evaluation, I first acquire a list of IPs that are verifiably
not being blocked by {\reflectors}. Since I own these IPs, I can easily verify
that by directly probing {\reflectors} from these IPs. I acquired and tested
1,265 IPs from five different /24s. Then I probe {\reflectors} and send the
spoofed packets with source addresses set to these pre-selected IPs. Since I
know that these IPs are not blocked, if I observe an IP ID increase sequence of
[+1, +1, +1, +1], then I know it is a false positive.

For false negative, I run the experiment with only probe packets, and no
spoofed packets. This is equivalent to the scenario where the host blocks the
spoofed IP. Then if I observe an IP ID increase sequence of [+1, +1, +6,
+6], then I know it was due to the background traffic at the {\reflector}
and hence is a false negative.

Although I have presented the design where I spoof five packets
in each of the last two seconds, I also experimented with a range of
numbers and calculated their false positive and negative rates. I tested
with spoofed packets equal to 3, 4, 5, 6, 7 respectively. For each number,
I use 15 distinct IPs I own as the source addresses to spoof, and I
create another group with 15 place holder IPs where I do not send spoofed
packets during the experiment. I run each experiment twice, and the final
results are shown in Figure~\ref{fig:fp_fn_analysis}.

%Our goal is to minimize the false positives and false negatives while also keeping
%in mind the amount of traffic I generate. Additionally, I have a few dimensions
%of the experiment which I can adjust to reach this goal. Namely, the number of
%packets I spoof, and the number of times I spoof.
%To find these optimal numbers, I test with a range of numbers and calculate

\begin{figure}[t]
\centering
\includegraphics[width=0.85\columnwidth]{data_usage/images/false_positive_negative.pdf}
\caption{False positive rates and false negative rates of the technique when spoofing different amount of packets.}
\label{fig:fp_fn_analysis}
\end{figure}

A few things stand out. The false negative rate drops significantly when I
send 5 spoofed packets. Surprisingly, the false negative rate jumps up
slightly when I spoof 6 packets. On the other hand, the false positive rate
keeps marginally trending downwards as I increase the number of spoofed
packets. I try to make the trade-off between having low false positive and
negative rates, but at the same time generating as little traffic as possible.
I choose 5 spoofed packets as a balance. By sending 5 spoofed packets, I
get a false positive rate of 2.5e-5, and a false negative rate of 8.5e-5.

Furthermore, I also experimented with strategies where I send 4 probe packets, from which
I get 3 IP ID deltas, and sending 6 probe packets, from which I get 5 IP ID
deltas. With only 3 deltas I suffer a higher false negative rate, as it is
easier for the {\reflector} to show the same IP ID increase sequence with
extra traffic. With 6 probes, on the other hand, I prolong the experiment,
and more importantly, it is harder for us to get the ``perfect signal'' since
it is harder to capture a period with no other traffic when the time window
is longer. Thus, the choice of 5 probe packets with 5 spoofed packets in
between is a trade-off between multiple factors.

%I choose to send 5 probe packets because it is a balance between getting
%enough sample point of IP ID to see the changes, and not sending too many
%packets to the {\reflector} that might affect the host, as I need to repeat
%the experiment many times for many different blacklist IPs. The first three
%probes (where I get the first two IP ID delta) act as a control, and the
%last two probe and the spoof packets serve the actually experiment. The
%choice of 5 spoofed packets between each second is also the result of
%balance. If I send too little spoofed packets, then it is hard to argue
%whether the additional IP ID increments come from our spoofed packets or the
%hosts' own third-party traffic. If I send too many spoofed packets, it is
%hard to ensure they will arrive the host exactly between the probe packets,
%and also might trigger some stateful firewall logic, as I discussed in
%Section~\ref{sec:requirement}.

\subsection{Control Group}
\label{sec:methvalid}
%\noteby{KL}{Here we
%    should describe additional validation, such as testing whether popular sites
%    are being blocked, and any other validation steps, such as repeating the
%    measurement, that I do. The part where I confirm our findings by asking
%    universities about their blocking policy can remain in the analysis section.}
To further validate the measurements, every time I test a set of
blacklist IPs against each reflector, I also include a control group
of 20 randomly chosen IPs. These IPs are chosen from networks
geo-located in the United States. I further ensure they do not
appear on any of the blacklists, they are BGP routable, and they are
not from the same ASes as the {\reflectors}. The purpose of the
control group is to create a random set of IPs that are unlikely to be
blocked in bulk by a {\reflector}. I use US IPs to avoid the
potential problem of geo-blocking. If a {\reflector} does block a
significant fraction of control IPs, it is probably because the
{\reflector} is not suitable for this methodology (one reason can be
that the ingress-filtering step did not catch these IPs). I discover
91 reflectors that constantly show blocking behavior for all control
IPs, while the remaining reflectors never block more than 10 of the
control IPs. I conclude that these 91 reflectors are not useful for
measurement, and remove them from the total reflector set.

\subsection{Ethical Considerations}
\label{sec:ethics}

In general, I believe the measurement would not cause much harm, as I only 
send TCP SYN-ACK packets to these hosts, without any other communication. Since
the reflectors are all active hosts on the public Internet, I believe it is
unlikely that any dramatic action will be taken if they only received SYN-ACK
packets but no other communication from blacklist IPs. I limited my
measurement within the United States to further reduce the potential impact.
However, admittedly the experiment may trigger security alerts, causing 
network administrators to spend time looking into these unnecessary alerts.
But to be further cautious, I take effort to minimize the
measurement packets I send to each reflector, as described in
Section~\ref{sec:methtrain}. Moreover, I will not disclose the exact
identity of any reflector in this chapter, and only report aggregated
numbers.

%\subsection{Tor Blacklist Placeholder}
%Note that the {\ettor} includes IPs from all nodes involved in Tor
%networks, including entry nodes, so the {\reflectors} who block this
%list cannot use Tor services themselves.  As a result, this Tor list
%is more broad than Tor exit blocking, in which a host only blocks exit
%nodes so other hosts cannot access their service from Tor (but the
%blocking host can still access the Tor network itself).
%\noteby{GV}{Why define the Tor blacklist here?  Perhaps move to
%  blacklist selection or validation?}
