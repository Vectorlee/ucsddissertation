\chapter{Threat Intelligence Data Study}
\label{chapter:data_character}

As described in the Introduction 
chapter, there is a surge of threat intelligence products in the 
recent years, and they all have eye-catching promises like high 
accuracy and good coverage. However, it is unclear whether the
existing products on the market can actually live up to the 
promises, and there has been little empirical assessment of 
threat intelligence data or even a consensus about what such an
evaluation would entail. 

A thorough data characteristics analysis is critical for the 
community to understand the patterns and limitations of the
existing threat intelligence products. After all, one need to 
understand the current situation before discussing how to 
improve it. Another motivation for this 
study is that since there is little empirical analysis of the 
data before, security community do not even have a set of metrics for
measuring and comparing different threat intelligence products. 
Thus, consumers of threat intelligence products have limited means 
to compare offerings or to factor the cost of such products into 
any model of the benefit to operational security.

These issue motivates my study to try to provide a grounded,
empirical footing for addressing such questions. 
In this chapter, I will talk about my work on data characteristics
study on threat intelligence. In particular, this chapter includes 
the following key points:
\begin{prettylist}
\item Introduced a set of basic \emph{threat intelligence metrics}
and describe a methodology for measuring them, notably: 
\veryemph{Volume},
\veryemph{Differential Contribution}, \veryemph{Exclusive Contribution},
\veryemph{Latency}, \veryemph{Coverage} and \veryemph{Accuracy}.
\item Analyze \numipfeeds\ distinct IP address \ti\ sources covering
six categories of threats and \numhashfeeds\ distinct malware file hash
\ti\ sources, and report their metrics.
\item Demonstrated techniques to evaluate the accuracy and coverage of
certain categories of \ti\ sources.
\item I conduct the analyses in two different time periods two 
years apart, and demonstrate the strong consistency between the 
findings.
\end{prettylist}

From the analysis, I find that while a few \ti\ data sources show
significant overlap, most do not.  This result is consistent with the
hypothesis advanced by~\cite{thomas2016abuse} that different kinds of
monitoring infrastructure will capture different kinds of attacks, but
I demonstrated it in a much broader context. This also revealed
that underlying this issue are broader limitations of \ti\ sources in
terms of coverage (most indicators are unique) and accuracy (false
positives may limit how such data can be used operationally).
Finally, I will present a longitudinal analysis suggesting that these
findings are consistent over time.

%\input{content/introduction}
\input{data_character/content/overview}
\input{data_character/content/metrics}
\input{data_character/content/ip-analysis}
\input{data_character/content/hash-analysis}
\input{data_character/content/new_vs_old}
\input{data_character/content/abs_latency}
\input{data_character/content/discussion}
%\input{data_character/content/background}
%\input{data_character/content/conclusion}