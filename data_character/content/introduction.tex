\section{Introduction}
\label{sec:intro}

Computer security is an inherently adversarial discipline in which
each ``side'' seeks to exploit the assumptions and limitations of the
other.  Attackers rely on exploiting knowledge of vulnerabilities,
configuration errors or operational lapses in order to penetrate
targeted systems, while defenders in turn seek to improve their
resistance to such attacks by better understanding the nature of
contemporary threats and the technical fingerprints left by attacker's
craft.  Invariably, this means that attackers are driven to innovate
and diversify while defenders, in response, must continually monitor
for such changes and update their operational security practices
accordingly.  This dynamic is present in virtually every aspect of the
operational security landscape, from anti-virus signatures to the
configuration of firewalls and intrusion detection systems to incident
response and triage.  Common to all such reifications, however, is the
process of monitoring for new data on attacker behavior and using that
data to update defenses and security practices. Indeed, the extent to
which a defender is able to gather and analyze such data effectively
defines a de facto window of vulnerability---the time during which an
organization is less effective in addressing attacks due to ignorance
of current attacker behaviors.

This abstract problem has given rise to a concrete demand for
contemporary threat data sources that are frequently collectively
referred to as \emph{threat intelligence} (\ti).  By far the most
common form of such data are so-called \emph{indicators of
  compromise:} simple observable behaviors that signal that a host or
network may be compromised.  These include both network indicators
such as IP addresses (e.g., addresses known to launch particular
attacks or host command-and-control sites, etc.) and file hashes
(e.g., indicating a file or executable known to be associated with a
particular variety of malware).  The presence of such indicators is a
symptom that alerts an organization to a problem, and part of an
organization's defenses might reasonably include monitoring its assets
for such indicators to detect and mitigate potential compromises as
they occur.

While each organization naturally collects a certain amount of threat
intelligence data on its own (e.g., the attacks they repel, the e-mail
spam they filter, etc.) any single entity has a limited footprint and
few are instrumented to carefully segregate crisp signals of attacks
from the range of ambiguity found in normal production network and
system logs.  Thus, it is now commonly accepted that threat
intelligence data procurement is a specialized activity whereby
third-party firms, and/or collections of public groups, employ a range
of monitoring techniques to aggregate, filter and curate quality
information about current threats.  Indeed, the promised operational
value of threat intelligence has created a thriving (multi-billion
dollar) market~\cite{timarket}. Established security companies with
roots in anti-virus software or network intrusion detection now offer
threat intelligence for sale, while some vendors specialize in threat
intelligence exclusively, often promising coverage of more
sophisticated threats than conventional sources.

Unfortunately, in spite of this tremendous promise, there has been
little empirical assessment of threat intelligence data or even a
consensus about what such an evaluation would entail.  Thus, consumers
of \ti\ products have limited means to compare offerings
or to factor the cost of such products into any model of the benefit
to operational security that might be offered.

This issue motivates our work to provide a grounded,
empirical footing for addressing such questions.  In particular, this
paper makes the following contributions:
\begin{prettylist}
\item We introduce a set of basic \emph{threat intelligence metrics}
and describe a methodology for measuring them, notably: \veryemph{Volume},
\veryemph{Differential Contribution}, \veryemph{Exclusive Contribution},
\veryemph{Latency}, \veryemph{Coverage} and \veryemph{Accuracy}.
\item We analyze \numipfeeds\ distinct IP address \ti\ sources covering
six categories of threats and \numhashfeeds\ distinct malware file hash
\ti\ sources, and report their metrics.
\item We demonstrate techniques to evaluate the accuracy and coverage of
certain categories of \ti\ sources.
\item We conduct the analyses in two different time periods two years apart,
and demonstrate the strong consistency between the findings.
\end{prettylist}

From our analysis, we find that while a few \ti\ data sources show
significant overlap, most do not.  This result is consistent with the
hypothesis advanced by~\cite{thomas2016abuse} that different kinds of
monitoring infrastructure will capture different kinds of attacks, but
we have demonstrated it in a much broader context.  We also reveal
that underlying this issue are broader limitations of \ti\ sources in
terms of coverage (most indicators are unique) and accuracy (false
positives may limit how such data can be used operationally).
Finally, we present a longitudinal analysis suggesting that these
findings are consistent over time.

%The remainder of this paper is structured as follows: we present our
%data collection and measurement metrics in
%Section~\ref{sec:overview}, then present results for result IP and
%file hash threat intelliegence indicators in
%Sections~\ref{sec:ip-analysis} anppd~\ref{sec:hash-analysis}
%respectively. In Section~\ref{sec:new_vs_old} we explore changes in
%these results over time and in Section~\ref{sec:abs_latency} we
%explore reporting latency.  We review these findings and the
%limitations they highlight in Section~\ref{sec:discussion} followed
%by a review of prior work Section~\ref{sec:background}, concluding in
%Section~\ref{sec:conclusion}.
