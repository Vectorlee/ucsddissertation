\section{Conclusion}
\label{sec:conclusion}

This paper has focused on the simplest, yet fundamental, metrics about
threat intelligence data. Using the proposed metrics, we measured a broad
set of \ti\ sources, and reported the characteristics and limitations of
\ti\ data. In addition to the individual findings mentioned in each section, here
we highlight the high-level lessons we learned from our study:

%---effectively investigating the extent to which disparate
%sources are measuring the same underlying empirical phenomena and can
%be meaningfully used to directly anticipate future attacks.  Thus far
%the evidence for this goal is spartan at best.

\begin{itemize}
	\item \ti\ feeds, far from containing
    homogeneous samples of some underlying truth, vary tremendously in the
    kinds of data they capture based on the particularities of their
    collection approach. Unfortunately, few \ti\ vendors explain the
    mechanism and methodology by which their data are collected and thus
    \ti\ consumers must make do with simple labels such as
    ``scan'' or ``botnet'', coupled with inferences about the
    likely mode of collection. Worse, a significant amount of data does not
    even have a clear definition of category, and is only labelled as
    ``malicious'' or ``suspicious'', leaving the ambiguity to consumers to
    decide what action should be taken based on the data.

    \item There is little evidence
    that larger feeds contain better data, or even that there are
    crisp quality distinctions between feeds across different categories
    or metrics (i.e., that a \ti\ provider whose feed performs well on one
    metric will perform well on another, or that these rankings will hold
    across threat categories). How data is collected also does not
    necessarily imply the feeds' attributes. For example, crowdsourcing-based feeds (\eg\ Badips feeds), are not always slower in reporting data
    than the self-collecting feeds (like \feedetiprep).

    \item Most IP-based \ti\ data sources are collections of
    singletons (i.e., that each IP address appears in at most one source)
    and even the higher-correlating data sources frequently have
    intersection rates of only 10\%. Moreover, when comparing with broad
    sensor data in known categories with broad effect (e.g., random
    scanning) fewer than 2\% of observed scanner addresses appear in most of
    the data sources we analyzed; indeed, even when focused on the largest
    and most prolific scanners, coverage is still limited to 10\%. There
    are similar results for file hash-based sources with little overlap
    among them.
\end{itemize}

The low intersection and coverage of \ti\ feeds could be the result of
%Taken together these suggest
several non-exclusive possibilities.
First is that the underlying space of indicators (both IP addresses
and malicious file hashes) is large and each individual data source
can at best sample a small fraction thereof.  It is almost certain
that this is true to some extent.  Second, different collection
methodologies---even for the same threat category---will select for
different sub distributions of the underlying ground truth data.
Third, this last effect is likely exacerbated by the fact that not all
threats are experienced uniformly across the Internet and, thus,
different methodologies will skew to either favor or disfavor targeted
attacks.


\begin{comment}
While resolving this question is of academic interest, it seems clear
that blindly using \ti\ data---even if one could afford to acquire
many such sources---is unlikely to be effective.  Proactive defenses,
such as blocking network connections or file executions that match
\ti\ indicators, are predicated on knowing, in advance, a large
fraction of the universe of future indicators; the evidence is that
this is not currently possible.  While there may be some utility in
doing this even for the small percentage of malicious traffic that may
be identified, this must be balanced against the costs of obtaining
and managing \ti\ data as well as the risks associated with false
positives.
\end{comment}


Based on our experience analyzing \ti\ data, we try to provide several
recommendations for the security community on this topic moving forward:

\begin{itemize}
    \item The threat intelligence community should standardize data labeling,
    with a clear definition of what the data means and how the data is collected. Security experts can then assess
    whether the data fit their need and the type of action should be taken
    on this data.

    \item There are few rules of thumb in selecting among \ti\ feeds,
    as there is not a clear correlation between different feed properties.
    Consumers need empirical metrics, such as those we describe, to
    meaningfully differentiate data sources, and to prioritize certain metrics
    based on their specific need.

    \item Blindly using \ti\ data---even if one could afford to acquire
    many such sources---is unlikely to provide better coverage and is
    also prone to collateral damage caused by false positives. Customers
    need to be always aware of these issues when deciding what action
    should be taken on this data.

    \item Besides focusing on the \ti\ data itself, future work should investigate
    the operational uses of threat intelligence in industry, as
    the true value of \ti\ data can only be understood in operational scenarios.
    Moreover, the community should explore more potential ways of using the data,
    which will extend our understanding of threat intelligence and also influence
    how vendors are curating the data and providing the services.

\end{itemize}


%However, this is only one potential use of threat intelligence data.
There are many ways we can use threat intelligence data.
It can be used to \emph{enrich} other information
(e.g., for investigating potential explanations of a security
incident), as a probabilistic canary (i.e., identifying an overall
site vulnerability via a single matching indicator may have value even
if other attacks of the same kind are not detected) or in providing a
useful source of ground truth data for supervised machine learning
systems. However, even given
such diverse purposes, organizations still need some way to prioritize which
\ti\ sources to invest in.  Our metrics provide some direction for
such choices. For example, an analyst who expects to use \ti\ interactively
during incident response would be better served by feeds with higher coverage,
but can accommodate poor accuracy, while an organization trying to
automatically label malicious instances for training purposes (e.g.,
brute force attacks) will be better served by the converse.  Thus, if
there is hope for demonstrating that threat intelligence can
materially impact operational security practices, we believe it will
be found in these more complex uses cases and that is where future
research will be most productive.
